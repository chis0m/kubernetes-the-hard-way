## Creating Kubernetes Configuration Files - Kubeconfig
`kubectl` client tool is required for this.
This task is automated by the `kubeconfig.sh` script

Typically, Kubeconfig is located at `${HOME}/.kube` as `config` file. It has three components, one or more `clusters`,
one or more `users` and one or more `contexts` and one `current-context`.
In a situation where you have other config files in .kube, you can set the config file to be used as kubeconfig by either setting the KUBECONFIG environment variable OR
specifying `--kubeconfig` flag in kubectl command e.g `kubectl config use-context %context-name%` --kubeconfig=kube-proxy.kubeconfig`

- cluster has the credentials like `name`, `CA` and `api server address`
- user has credentials like `name`, its `client certificate` and `client certificate key`
- context: this binds a `cluster` to a `user`. It as `name`, `user`, `cluster` and optionally, `namespace`   etc
- current-context: specifies the current active context. To change the current context, run `kubectl config use-context %context-name%`

### Requirements
- NAME of the infrastructure. In this case, I used `MC-K8`
- Master and Worker Node IP addresses  
- Kubernetes DNS address
```bash
KUBERNETES_API_SERVER_ADDRESS=$(aws elbv2 describe-load-balancers \
--names MC-K8-NetworkLoadBalancer \
--output text --query 'LoadBalancers[].DNSName')
```

#### Files
1. Generate the kubelet kubeconfig file
  - The client certificate of each kubelet will be used to generate its own kubeconfig file because these certificates has the node's ip address specified in the CN of the certificate.
  - This will also ensure that appropriate authorization is applied to that node through the Node Authorizer
  - In creating a kubeconfig, you create the cluster, user and context
  - You can see that the `--server` flag shows that the worker node will route through the loadbalancer to talk to the master node
```bash
for i in 0 1 2; do

instance="${NAME}-Cluster-Worker-${i}"
instance_hostname="ip-10-0-0-2${i}"

 # Set the kubernetes cluster in the kubeconfig file
  kubectl config set-cluster ${NAME} \
    --certificate-authority=pki/ca/ca.pem \
    --embed-certs=true \
    --server=https://$KUBERNETES_API_SERVER_ADDRESS:6443 \
    --kubeconfig=kubeconfig/${instance}.kubeconfig

# Set the user credentials in the kubeconfig file
  kubectl config set-credentials system:node:${instance_hostname} \
    --client-certificate=pki/kubelets/${instance}.pem \
    --client-key=pki/kubelets/${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=kubeconfig/${instance}.kubeconfig
#
# Set the context in the kubeconfig file
  kubectl config set-context default \
    --cluster=${NAME} \
    --user=system:node:${instance_hostname} \
    --kubeconfig=kubeconfig/${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=kubeconfig/${instance}.kubeconfig
done
```

2. Generate the kube-proxy kubeconfig
```bash
kubectl config set-cluster ${NAME} \
  --certificate-authority=pki/ca/ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_API_SERVER_ADDRESS}:6443 \
  --kubeconfig=kubeconfig/kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
  --client-certificate=pki/kubeproxy/kube-proxy.pem \
  --client-key=pki/kubeproxy/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kubeconfig/kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=${NAME} \
  --user=system:kube-proxy \
  --kubeconfig=kubeconfig/kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

3. Generate the Kube-Controller-Manager kubeconfig
- The server (--server ) address here will be `127.0.0.1` localhost because the controller is located in same node as the kube-api server

```bash
kubectl config set-cluster ${NAME} \
  --certificate-authority=pki/ca/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kubeconfig/kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=pki/controller/kube-controller-manager.pem \
  --client-key=pki/controller/kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kubeconfig/kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=${NAME} \
  --user=system:kube-controller-manager \
  --kubeconfig=kubeconfig/kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kubeconfig/kube-controller-manager.kubeconfig
```

4. Generating the Kube-Scheduler Kubeconfig
```bash
kubectl config set-cluster ${NAME} \
  --certificate-authority=pki/ca/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kubeconfig/kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=pki/scheduler/kube-scheduler.pem \
  --client-key=pki/scheduler/kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kubeconfig/kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=${NAME} \
  --user=system:kube-scheduler \
  --kubeconfig=kubeconfig/kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kubeconfig/kube-scheduler.kubeconfig
```

5. Finally, generate the kubeconfig file for the admin user

```bash
kubectl config set-cluster ${NAME} \
  --certificate-authority=pki/ca/ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_API_SERVER_ADDRESS}:6443 \
  --kubeconfig=kubeconfig/admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=pki/admin/admin.pem \
  --client-key=pki/admin/admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kubeconfig/admin.kubeconfig

kubectl config set-context default \
  --cluster=${NAME} \
  --user=admin \
  --kubeconfig=kubeconfig/admin.kubeconfig

kubectl config use-context default --kubeconfig=kubeconfig/admin.kubeconfig
```

## Copy the files to their servers

1. Master Node
We are copying the controller-manager, and scheduler config to the master node
```bash
for i in 0 1 2; do
instance="${NAME}-Cluster-Master-${i}" \
  external_ip=$(aws ec2 describe-instances \
    --filters "Name=tag:Name,Values=${instance}" \
    --output text --query 'Reservations[].Instances[].PublicIpAddress')
  scp -i ${SSH_KEY} \
    -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no \
    kubeconfig/kube-controller-manager.kubeconfig \
    kubeconfig/kube-scheduler.kubeconfig \
    ubuntu@${external_ip}:~/;
done

```

2. Worker Nodes
We are copying kube-proxy, admin and kubelets config to the worker nodes
```bash
for i in 0 1 2; do
  instance="${NAME}-Cluster-Worker-${i}"
  external_ip=$(aws ec2 describe-instances \
    --filters "Name=tag:Name,Values=${instance}" \
    --output text --query 'Reservations[].Instances[].PublicIpAddress')
  scp -i ${SSH_KEY} \
    -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no \
    kubeconfig/${instance}.kubeconfig \
    kubeconfig/kube-proxy.kubeconfig \
    kubeconfig/admin.kubeconfig \
    ubuntu@${external_ip}:~/; \
done
```

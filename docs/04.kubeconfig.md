## Creating Kubernetes Configuration Files - Kubeconfig
- Typically, Kubeconfig is located at `${HOME}/.kube` as `config` file. It has three components, one or more `clusters`,
  one or more `users` and one or more `contexts` and one `current-context`.
  In a situation where you have other config files in .kube, you can set the config file to be used as kubeconfig by either setting the `KUBECONFIG` environment variable OR
  specifying `--kubeconfig` flag in kubectl command e.g `kubectl config use-context %context-name%` --kubeconfig=kube-proxy.kubeconfig`
- cluster has the credentials like `name`, `CA` and `api server address`
- user has credentials like `name`, its `client certificate` and `client certificate key`
- context: this binds a `cluster` to a `user`. It as `name`, `user`, `cluster` and optionally, `namespace`   etc
- current-context: specifies the current active context. To change the current context, run `kubectl config use-context %context-name%`

### Variables
- NAME of the infrastructure. In this case, I used `MC-K8`
- Master and Worker Node IP addresses  
- Kubernetes DNS address
```bash
KUBERNETES_API_SERVER_ADDRESS=$(aws elbv2 describe-load-balancers \
--names MC-K8-NetworkLoadBalancer \
--output text --query 'LoadBalancers[].DNSName')
```

#### Files
1. Generate the kubelet kubeconfig file
  - The client certificate of each kubelet will be used to generate its own kubeconfig file because these certificates has the node's ip address specified in the CN of the certificate.
  - This will also ensure that appropriate authorization is applied to that node through the Node Authorizer
  - In creating a kubeconfig, you create the cluster, user and context
  - You can see that the `--server` flag shows that the worker node will route through the load balancer to talk to the master node
```bash
for i in 0 1 2; do

instance="${NAME}-Cluster-Worker-${i}"
instance_hostname="ip-10-0-0-2${i}"

 # Set the kubernetes cluster in the kubeconfig file
  kubectl config set-cluster ${NAME} \
    --certificate-authority=output/pki/ca/ca.pem \
    --embed-certs=true \
    --server=https://$KUBERNETES_API_SERVER_ADDRESS:6443 \
    --kubeconfig=output/kubeconfig/${instance}.kubeconfig

# Set the user credentials in the kubeconfig file
  kubectl config set-credentials system:node:${instance_hostname} \
    --client-certificate=output/pki/kubelets/${instance}.pem \
    --client-key=output/pki/kubelets/${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=output/kubeconfig/${instance}.kubeconfig
#
# Set the context in the kubeconfig file
  kubectl config set-context default \
    --cluster=${NAME} \
    --user=system:node:${instance_hostname} \
    --kubeconfig=output/kubeconfig/${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=output/kubeconfig/${instance}.kubeconfig
done
```

2. Generate the kube-proxy kubeconfig
Refer to the `kubeconfig.sh` script

3. Generate the Kube-Controller-Manager kubeconfig
- The server (--server ) address here will be `127.0.0.1` localhost because the controller manager is located in same node as the kube-api server

```bash
kubectl config set-cluster ${NAME} \
  --certificate-authority=output/pki/ca/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=output/kubeconfig/kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=output/pki/controller/kube-controller-manager.pem \
  --client-key=output/pki/controller/kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=output/kubeconfig/kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=${NAME} \
  --user=system:kube-controller-manager \
  --kubeconfig=output/kubeconfig/kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=output/kubeconfig/kube-controller-manager.kubeconfig
```

4. Generating the Kube-Scheduler Kubeconfig
Refer to the `kubeconfig.sh` script

5. Finally, generate the kubeconfig file for the admin user
   Refer to the `kubeconfig.sh` script
   

## Copy the files to their servers
1. Master Node
   We are copying the controller-manager, and scheduler config to the master node 
   Refer to the `copyconfig.sh` script


2. Worker Nodes
   We are copying kube-proxy, admin and kubelets config to the worker nodes
   Refer to the `copyconfig.sh` script

## Bootstrapping components on the worker nodes
1. SSH into each worker node
Check kubeconfig.md to see how
2. Install OS dependencies
```bash
sudo apt-get update
sudo apt-get -y install socat conntrack ipset
```

- socat: Socat is the default implementation for Kubernetes port-forwarding when using dockershim for the kubelet runtime.
- Dockershim: was a temporary solution proposed by the Kubernetes community to add support for Docker so that it could serve as its container runtime
- Conntrack: Allows the linux kernel to keep track of all logical network connections or flows.
  It is essential for performant complex networking of Kubernetes where nodes need to track connection information between thousands of pods and services
- ipset: is an extension to iptables which is used to configure firewall rules on a Linux server. 
  Kubernetes uses ipsets to implement a distributed firewall solution that enforces network policies within the cluster

3. Disable swap 
If `swap` is not disabled, kubelet will not start. It is highly recommended to allow Kubernetes to handle resource allocation
```bash
# check if swap is enabled
sudo swapon --show

# if enabled
sudo swapoff -a
```

4. Download and install a container runtime
We will be using `containerd` because kubernetes(v1.24) will stop the use of dockershim which provides support for docker in k8
   
```bash
# Download binaries for runc, cri-ctl, and containerd
 wget https://github.com/opencontainers/runc/releases/download/v1.0.0-rc93/runc.amd64 \
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.21.0/crictl-v1.21.0-linux-amd64.tar.gz \
  https://github.com/containerd/containerd/releases/download/v1.4.4/containerd-1.4.4-linux-amd64.tar.gz
  
# configure containerd
mkdir containerd
tar -xvf crictl-v1.21.0-linux-amd64.tar.gz
tar -xvf containerd-1.4.4-linux-amd64.tar.gz -C containerd
sudo mv runc.amd64 runc
chmod +x crictl runc  
sudo mv crictl runc /usr/local/bin/
sudo mv containerd/bin/* /bin/
sudo mkdir -p /etc/containerd/

cat << EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
EOF


# Create the containerd.service systemd unit file
cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF
```

5. Create directories for to configure `kubelet`, `kube-proxy`, `cni`, and a directory to keep the kubernetes root ca file
```bash
sudo mkdir -p \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubernetes \
  /var/run/kubernetes
```

6. Download and Install CNI 
- Container Network Interface CNI: is a set of standards and libraries that define how programs(plugins) should be developed to solve networking challenges in a Container Runtime Environment
- Kubernetes uses CNI as an interface between network providers and Kubernetes Pod networking
- kubelet invokes the CNI plugins. CNI plugins get called when we start containers   
```bash
wget -q --show-progress --https-only --timestamping \
  https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz
  
# Install CNI into /opt/cni/bin/. We will be using Flannel  
sudo tar -xvf cni-plugins-linux-amd64-v0.9.1.tgz -C /opt/cni/bin/
```

7. Download and install binaries for kubectl, kube-proxy, and kubelet
```bash
wget -q --show-progress --https-only --timestamping \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubelet
  
chmod +x  kubectl kube-proxy kubelet 
sudo mv  kubectl kube-proxy kubelet /usr/local/bin/ 
```

### Configure Components
1. Network
- The Pod Cidr is from terraform when we created the ec2 instances
-  It is very important to ensure that the CIDR does not overlap with Node Instances IP
- In this case I used 10.xx.xx.xx cidr group for Nodes and 172.16.xx.xx for Pods
- Regardless of which node is running the container in the cluster(3 Nodes), Kubernetes expects that all the containers must be able to communicate with each other
- To mitigate security risks and have a better controlled network topology, Kubernetes uses CNI (Container Network Interface) to manage Network Policies which can be 
  used to operate the Pod network through external plugins such as Calico, Flannel or Weave Net to name a few.
```bash
POD_CIDR=$(curl -s http://169.254.169.254/latest/user-data/ \
  | tr "|" "\n" | grep "^pod-cidr" | cut -d"=" -f2)
echo "${POD_CIDR}"
```
- You must decide on the Pod CIDR per worker node. Each worker node will run multiple pods, and each pod will have its own IP address
  In this set up the pod cidr are 10.200.0.0/24, 10.200.1.0/24, 10.200.2.0/24
  
- There must be `Bridge network` and `Virtual network` that the pods using the pod cidr
```bash
# Configure the bridge and loopback networks
cat > 10-200-bridge.conf <<EOF
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "${POD_CIDR}"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF

# Loopback
cat > 99-loopback.conf <<EOF
{
    "cniVersion": "0.3.1",
    "type": "loopback"
}
EOF

# Move the files to the network configuration directory:
sudo mv 172-20-bridge.conf 99-loopback.conf /etc/cni/net.d/
```


2. KUBELET
- Move the certificates and kubeconfig file to their respective configuration directories
```bash
NAME=MC-K8
WORKER_NAME=${NAME}-$(curl -s http://169.254.169.254/latest/user-data/ \
  | tr "|" "\n" | grep "^name" | cut -d"=" -f2)
echo "${WORKER_NAME}"

sudo mv ${WORKER_NAME}-key.pem ${WORKER_NAME}.pem /var/lib/kubelet/
sudo mv ${WORKER_NAME}.kubeconfig /var/lib/kubelet/kubeconfig
sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
sudo mv ca.pem /var/lib/kubernetes/
```

- Create the kubelet-config.yaml file and pass it to the kubelet systemd service
  Instead of using multiple startup flags for kubelet systemd file, we would create a yaml file and pass it as a config file
  when starting the service
```bash
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
resolvConf: "/etc/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${WORKER_NAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${WORKER_NAME}-key.pem"
EOF

# systemd service
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service
[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --cluster-domain=cluster.local \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF

```

3. KUBE PROXY - same way we did for kubelet
   Config file and the systemd service

```bash
export CLUSTER_CIDR=$(curl -s http://169.254.169.254/latest/user-data/ \
  | tr "|" "\n" | grep "^cluster-cidr" | cut -d"=" -f2)

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: ${CLUSTER_CIDR}
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF
```   

4. Enable and Restart Services
```bash
sudo systemctl daemon-reload
sudo systemctl enable containerd kubelet kube-proxy
sudo systemctl start containerd kubelet kube-proxy
```
